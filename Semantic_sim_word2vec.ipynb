{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebooks shows the basic staps for computing sematic similarity using Word2vec.\n",
    "Word2vec represents each word as a high-dimension vector of numbers (300 numbers) \n",
    "which capture relationships between words. \n",
    "Words which appear in similar contexts are mapped to vectors which are nearby as measured by cosine similarity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.models as models\n",
    "from scipy import spatial\n",
    "import os, csv, numpy, pandas\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, you need to find this \"GoogleNews-vectors-negative300.bin\" file online: \n",
    "https://github.com/mmihaltz/word2vec-GoogleNews-vectors\n",
    "and download it to your working folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the path location to match the file location\n",
    "\n",
    "word2vec = models.KeyedVectors.load_word2vec_format(\n",
    "    '/home1/noaherz/word2vec/GoogleNews-vectors-negative300.bin.gz', binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example demonstrates how to retrieve 300-dimensional vector representations (embeddings) \n",
    "for a list of words using a pre-trained Word2Vec model.\n",
    "\n",
    "We'll use four example items: \"cat\", \"dog\", \"bread\", and \"nonsensical12\". \n",
    "In practice, your code should iterate over a full list of words of interest.\n",
    "\n",
    "Note that some words (e.g., \"nonsensical12\") may not be present in the Word2Vec vocabulary. \n",
    "These items should be excluded from downstream analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nonsensical12\n",
      "(3, 300)\n"
     ]
    }
   ],
   "source": [
    "# How to get 300-dimensional word vectors?\n",
    "\n",
    "X = numpy.zeros(300)\n",
    "word_ids_in_analysis_title = []\n",
    "words_in_analysis_title = []\n",
    "words_that_are_in_w2c = []\n",
    "\n",
    "# Here, you would want to input all of your items (I usually do this from a csv, but having a dummy example here)\n",
    "word_list = [\"cat\", \"dog\", \"bread\", \"nonsensical12\"]\n",
    "word_list = np.array(word_list)\n",
    "\n",
    "for no, item in enumerate(word_list):\n",
    "    try:\n",
    "        X = numpy.vstack((X,preprocessing.normalize(word2vec[item].reshape(1, -1) )))\n",
    "        word_ids_in_analysis_title.append(no)\n",
    "        words_in_analysis_title.append(item)\n",
    "    except:\n",
    "        print(item)\n",
    "X = X[1:len(X)]\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.76094574\n",
      "1.0\n",
      "0.14007339\n"
     ]
    }
   ],
   "source": [
    "# How to get cosine theta similarity between two words?\n",
    "\n",
    "print(word2vec.similarity(\"cat\", \"dog\"))\n",
    "print(word2vec.similarity(\"cat\", \"cat\"))\n",
    "print(word2vec.similarity(\"cat\", \"bread\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "(3, 3)\n",
      "[[1.0, 0.76094574, 0.14007339], [0.76094574, 0.99999994, 0.17611265], [0.14007339, 0.17611265, 1.0]]\n"
     ]
    }
   ],
   "source": [
    "# How to create an NxN dimensional matrix for similarity?\n",
    "\n",
    "simmatrix = []\n",
    "\n",
    "word_list = word_list[:3] #I'm excluding the last word now\n",
    "\n",
    "for i in word_list:\n",
    "    this_word = []\n",
    "    for j in word_list:\n",
    "        this_word.append(word2vec.similarity(i, j))\n",
    "    simmatrix.append(this_word)\n",
    "\n",
    "# If all your words are in word2vec, the shape of the matrix should be len(word_pool)xlen(word_pool)\n",
    "# Otherwise, you'll get this error: KeyError: \"word 'doughnut' not in vocabulary\" and you'd need to find another item\n",
    "# You can save this simmatrix for future use with np.save() function\n",
    "\n",
    "print(len(word_list))\n",
    "print(np.shape(np.array(simmatrix)))\n",
    "print(simmatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CAT and its similairty to other items:\n",
      "cat 1.0\n",
      "dog 0.76094574\n",
      "bread 0.14007339\n"
     ]
    }
   ],
   "source": [
    "# Let's do a sanity check with the word cat and it's similarity to other items\n",
    "\n",
    "print(word_list[0].upper(), \"and its similairty to other items:\")\n",
    "for no, i in enumerate(simmatrix[0]):    \n",
    "    print(word_list[no], i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.0, 0.760945707266373, 0.14007337807192446], [0.760945707266373, 1.0, 0.17611264178511324], [0.14007337807192446, 0.17611264178511324, 1.0]]\n"
     ]
    }
   ],
   "source": [
    "# You can also obtain the similarities directly from the 300-dimensional word vectors\n",
    "\n",
    "simmatrix_cos = []\n",
    "\n",
    "for i in X:\n",
    "    this_word_cos = []\n",
    "    for j in X:\n",
    "        this_word_cos.append(numpy.dot(i,j)/(numpy.linalg.norm(i)* numpy.linalg.norm(j)))\n",
    "    simmatrix_cos.append(this_word_cos)\n",
    "    \n",
    "print(simmatrix_cos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# And confirm the matrices are identical :) \n",
    "\n",
    "simmatrix == simmatrix_cos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "The Word2Vec model distinguishes between uppercase and lowercase characters. For example, \"CAT\" may not be found in the model even if \"cat\" is present.\n",
    "\n",
    "To ensure consistent matching, it is advisable to standardize all words to lowercase (or uppercase, as long as it is consistent) before retrieving their vector representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def case_insensitive_similarity(word1, word2, word2vec):\n",
    "        \"\"\"\n",
    "    Compute Word2Vec similarity between two words in a case-insensitive way.\n",
    "    Tries all combinations of upper/lower case and returns the maximum similarity score.\n",
    "\n",
    "    Args:\n",
    "        word1 (str): First word.\n",
    "        word2 (str): Second word.\n",
    "        word2vec: A trained Word2Vec model.\n",
    "\n",
    "    Returns:\n",
    "        float or None: Maximum similarity score across case combinations,\n",
    "                       or None if neither word is found in the model.\n",
    "    \"\"\"\n",
    "    similarities = []\n",
    "    cases = [(word1.lower(), word2.lower()),\n",
    "             (word1.lower(), word2.upper()),\n",
    "             (word1.upper(), word2.lower()),\n",
    "             (word1.upper(), word2.upper())]\n",
    "\n",
    "    for w1, w2 in cases:\n",
    "        try:\n",
    "            similarities.append(word2vec.similarity(w1, w2))\n",
    "        except KeyError:\n",
    "            continue\n",
    "\n",
    "    return max(similarities) if similarities else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between 'Cat' and 'dog': 0.7609\n"
     ]
    }
   ],
   "source": [
    "# Example usage of case_insensitive_similarity function\n",
    "word_a = \"Cat\"\n",
    "word_b = \"dog\"\n",
    "\n",
    "# Compute similarity\n",
    "similarity_score = case_insensitive_similarity(word_a, word_b, word2vec)\n",
    "print(f\"Similarity between '{word_a}' and '{word_b}': {similarity_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "cml3",
   "language": "python",
   "name": "cml3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
